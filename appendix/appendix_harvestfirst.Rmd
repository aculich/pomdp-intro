---
title: "Appendix A: Code and supplementary figures for simulation and policy analyis"
author: "Carl Boettiger"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: ../manuscripts/citations.bib

---

# Overview

1. Model definition:
2. Deterministic Solutions
3. Discretization and the POMDP solution
4. Simulation 

```{r knitr, include=FALSE}
knitr::opts_chunk$set(comment="", message=FALSE, warning = FALSE, cache=TRUE, dev="cairo_pdf")
```


```{r library, message=FALSE}
## install package first if necessary.
# devtools::install_github("boettiger-lab/sarsop") 
library(sarsop)       # the main POMDP package
library(tidyverse)    # for munging and plotting
library(parallel)
library(gridExtra)
```

```{r optional_plotting_config, include=FALSE}
library(ggthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(hrbrthemes)

extrafont::loadfonts()
hrbrthemes::import_roboto_condensed() 
ggplot2::theme_set(hrbrthemes::theme_ipsum_rc())

## Set colors
#palette <- ptol_pal()(6)
palette <- c("#D9661F", "#3B7EA1",  "#6C3302", "#FDB515", "#00B0DA",  "#CFDD45")
colors <- set_names(c(palette[c(1:4,2,5)], "grey", "black", palette[6]), 
                    c("TAC", "POMDP", "MSY", "CE", 
                      "POMDP: low prior",
                      "POMDP: high prior",
                      "biomass",
                      "catch",
                      "SDP"))
## Set as default palette so we can omit scale_color_manual in every plot call
## https://stackoverflow.com/a/44307181/258662
scale_colour_discrete <- function(...) scale_colour_manual(..., values=colors)
scale_fill_discrete <- function(...) scale_fill_manual(..., values=colors)
```

```{r}
options(mc.cores=parallel::detectCores())
log_dir <- "../data/observe_harvest_recruit" # Store the computed solution files here

```



# Basic deterministic model

```{r}
r <- 0.75
K <- 1

## Unlike classic Graham-Schaefer, this assumes harvest 
## occurs right after assessment, before recruitment
f <- function(x, h){ 
    s <- pmax(x-h, 0) 
    s + s * r * (1 - s / K) 
}
```

Utility (reward) function.  (Note that setting a smaller discount will take longer to converge to smooth POMDP solution, but result in `S_star` closer to the simple `B_MSY`.)  

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.95
```


# Calculating MSY & TAC

This uses a generic optimization routine to find the stock size at which the maximum growth rate is achieved.  (Note that this also depends on the discount rate since future profits are worth proportionally less than current profits).

```{r}
## A generic routine to find stock size (x) which maximizes 
## growth rate (f(x,0) - x, where x_t+1 = f(x_t))
S_star <- optimize(function(x) -f(x,0) + x / discount, 
                   c(0, 2*K))$minimum
S_star
```


Since we `observe` -> `harvest` -> `recruit`, we would observe the stock at its pre-harvest size, $X_t \sim B_{MSY} + H_{MSY}$.

```{r}
#B_MSY <- S_star     # recruit first, as in classic Graham-Schaefer
B_MSY <- f(S_star,0) # harvest first, we observe the population at B_MSY + h

#MSY <- f(B_MSY,0) - B_MSY  # recruit first
MSY <- B_MSY - S_star  # harvest first

F_MSY <- MSY / B_MSY  
F_TAC = 0.8 * F_MSY
```


As a basic reference point, simulate these three policies in a purely deterministic world.  Unlike later simulations, here we consider all states an actions exactly (that is, within floating point precision).  Later, states and actions are limited to a discrete set, so solutions can depend on resolution and extent of that discretization. 

```{r}
MSY_policy <- function(x) F_MSY * x
TAC_policy <- function(x) F_TAC * x

## ASSUMES harvest takes place *before* recruitment, f(x-h), not after.
escapement_policy <- function(x) pmax(x - S_star,0)  

x0 <- K/6
Tmax = 100
do_det_sim <- function(policy, f, x0, Tmax){
    action <- state <- obs <- as.numeric(rep(NA,Tmax))
    state[1] <- x0

    for(t in 1:(Tmax-1)){
      action[t] <- policy(state[t])
      obs[t] <- state[t] - action[t]  # we observe->harvest->recruitment
      state[t+1] <- f(state[t], action[t]) 
    }
    data.frame(time = 1:Tmax, state, action, obs)
  }

det_sims <- 
  list(MSY = MSY_policy, 
       TAC = TAC_policy,
       CE = escapement_policy) %>% 
  map_df(do_det_sim, 
         f, x0, Tmax, 
         .id = "method") 

write_csv(det_sims, "../data/observe_harvest_recruit/det_sims.csv")
```


With no stochasticity, MSY leads to the same long-term stock size as under the constant escapement rule, but takes longer to get there.  This level is essentially $B_{MSY}$, though because the model considered here implements events in the order: *observe, harvest, recruit*; rather than *observe, recruit, harvest*, we see the stock at the pre-harvest size of $B_{MSY} + H_{MSY}$ (that is, $K/2 + rK/4$).   More conservative rules, such as a harvest set to 80% of MSY result in faster recovery of the stock than under MSY, but slower than under constant escapement.  Due to the reduced maximum harvest rate, such rules lead to stock returning to a value higher than $B_{MSY}$.  


```{r fig.width=5, fig.height=5}
det_sims %>%
  ggplot(aes(time, state, col=method)) + 
  geom_line(lwd=1) + 
  coord_cartesian(ylim = c(0, 1)) + 
  theme(legend.position = "bottom") + 
  ylab("Mean biomass")
```

-----------


#  Introduce a discrete grid

```{r}
## Discretize space
states <- seq(0,2, length=100)
actions <- states
observations <- states
```

We compute the above policies on this grid for later comparison.


```{r}
index <- function(x, grid) map_int(x, ~ which.min(abs(.x - grid)))


policies <- data.frame(
#  CE = index(pmax(f(states,0) - S_star,0), actions), # obs,recruit,harv, f(x_t) - h_t
  CE = index(escapement_policy(states),     actions), # obs,harv,recruit, f(x_t - h_t)
  MSY = index(MSY_policy(states),               actions),
  TAC = index(TAC_policy(states),               actions))
```



# POMDP Model

We compute POMDP matrices for a range of `sigma_g` and `sigma_m` values.  (We consider `sigma_g = 0.02` rather than precisely zero here to avoid a degeneracy in the discretized tranisition matrix. Calculations for the precise deterministic solution can be solved without discretization as shown above, and match the results of this nearly-zero nosie level in the discrete simulation, as expected.)


```{r}
meta <- expand.grid(sigma_g = c(0.02, 0.1, 0.15), 
                    sigma_m = c(0, 0.1, 0.15),
                    stringsAsFactors = FALSE) %>%
        mutate(scenario  = as.character(1:length(sigma_m)))
meta
```


```{r}
models <- 
  parallel::mclapply(1:dim(meta)[1], 
           function(i){
  fisheries_matrices(
  states = states,
  actions = actions,
  observed_states = observations,
  reward_fn = reward_fn,
  f = f,
  sigma_g = meta[i,"sigma_g"][[1]],
  sigma_m = meta[i,"sigma_m"][[1]],
  noise = "normal")
})

```


## POMDP solution

The POMDP solution is represented by a collection of alpha-vectors and values, returned in a `*.policyx` file.
Each scenario (parameter combination of `sigma_g`, `sigma_m`, and so forth) results in a separate solution file.

Because this solution is computationally somewhat intensive, be sure to have ~ 4 GB RAM per core if running the 9 models in parallel.  Alternately, readers can skip the evaluation of this code chunk and read the cached solution from the `policyx` file using the `*_from_log` functions that follow:

```{r eval=FALSE}
dir.create(log_dir)
system.time(
  alphas <- 
    parallel::mclapply(1:length(models), 
    function(i){
      log_data <- data.frame(model = "gs", 
                             r = r, 
                             K = K, 
                             sigma_g = meta[i,"sigma_g"][[1]], 
                             sigma_m = meta[i,"sigma_m"][[1]], 
                             noise = "normal",
                             scenario = meta[i, "scenario"][[1]])
      
      sarsop(models[[i]]$transition,
             models[[i]]$observation,
             models[[i]]$reward,
             discount = discount,
             precision = 0.00000002,
             timeout = 15000,
             log_dir = log_dir,
             log_data = log_data)
    })
)

```

We can read the stored solution from the log:

```{r}
meta <- meta_from_log(data.frame(model="gs", discount=discount), log_dir) %>% 
  mutate(scenario = as.character(scenario)) %>%
  left_join(meta) %>% 
  arrange(scenario)
alphas <- alphas_from_log(meta, log_dir)
```


------

# Simulations

Having calculated the POMDP solution in terms of these `alpha` vectors, we can easily compute the POMDP policy for any given prior and update the prior given further observations according to Bayes rule.  We assume a uniform prior belief at the start of the POMDP simulation.  These tasks are perfromed by the `sim_pomdp` fucntion. Simulating the static strategies (CE, MSY, TAC) is easier, since the policy is uniquely determined by the observed state; we simply need to simulate the stochastic growth and process and measurement with error at each time step an then compute the corrsponding policy. The `sim_pomdp` function performs such static simulations when given a pre-specified policy instead of a set of `alpha` vectors.  

## Simulating the static policies under uncertainty

```{r static_sims}
set.seed(12345)

Tmax <- 100
x0 <- which.min(abs(K/6 - states))
reps <- 100
static_sims <- 
 map_dfr(models, function(m){
            do_sim <- function(policy) sim_pomdp(
                        m$transition, m$observation, m$reward, discount, 
                        x0 = x0, Tmax = Tmax, policy = policy, reps = reps)$df
            map_dfr(policies, do_sim, .id = "method")
          }, .id = "scenario") 

```

## Simulating the POMDP policies under uncertainty

```{r pomdp_sims}
set.seed(12345)

unif_prior <- rep(1, length(states)) / length(states)
pomdp_sims <- 
  map2_dfr(models, alphas, function(.x, .y){
             sim_pomdp(.x$transition, .x$observation, .x$reward, discount, 
                       unif_prior, x0 = x0, Tmax = Tmax, alpha = .y,
                       reps = reps)$df %>% 
              mutate(method = "POMDP") # include a column labeling method
           },
           .id = "scenario")
```


We then combine the resulting data frames, transition (mutate) the units from grid indices (`state`, `action` $\in 1 ... N$) to continuous values (`states`, `actions` $\in [0,2]$), and select necessary columns before writing the data out to a file.  

```{r}
sims <- bind_rows(static_sims, pomdp_sims) %>%
    left_join(meta) %>%  ## include scenario information (sigmas; etc)
    mutate(state = states[state], action = actions[action]) %>%
    select(time, state, rep, method, sigma_m, sigma_g, value)

write_csv(sims, file.path(log_dir, "sims.csv"))
```


##  Figure S2

We the results varying over different noise intensities, `sigma_g`, and `sigma_m`.  Note that Figure 1 of the main text shows only the case of `sigma_g = 0.15`, `sigma_m = 0.1` and omits MSY to simplify the presentation.

```{r}
sims <- read_csv(file.path(log_dir, "sims.csv"), col_types = "inicnnn")

sims  %>%
  select(time, state, rep, method, sigma_m, sigma_g) %>%
  group_by(time, method, sigma_m, sigma_g) %>%
  summarise(mean = mean(state), sd = sd(state)) %>%
  ggplot(aes(time, mean, col=method, fill=method)) + 
  geom_line() + 
  geom_ribbon(aes(ymax = mean + sd, ymin = mean-sd), col = NA, alpha = 0.1) +
  facet_grid(sigma_m ~ sigma_g, 
             labeller = label_bquote(sigma[m] == .(sigma_m),
                                     sigma[g] == .(sigma_g))) + 
  coord_cartesian(ylim = c(0, 1))

```

##  Figure S3: Economic value

Economic value is shown under varying noise levels under each strategy.

```{r}
sims %>%
  select(time, value, rep, method, sigma_m, sigma_g) %>%
  filter(sigma_g %in% c("0.1", "0.15")) %>%
  group_by(rep, method, sigma_m, sigma_g) %>%
  summarise(npv = sum(value)) %>%  
  group_by(method, sigma_m, sigma_g) %>%
  summarise(net_value = mean(npv), se = sd(npv) / mean(npv)) %>%
  ungroup() %>%

  ggplot(aes(method, net_value, ymin=net_value-se, ymax=net_value+se, fill=method)) + 
  geom_bar(position=position_dodge(), stat="identity") + 
  geom_errorbar(size=.3,width=.2,
                position=position_dodge(.9)) +
  facet_grid(sigma_m~sigma_g, 
             labeller = label_bquote(sigma[m] == .(sigma_m),
                                     sigma[g] == .(sigma_g))) +
  theme(legend.position = "none") + 
  ylab("expected net present value")


```




## POMDP simulations overestimating measurement uncertainty

While the POMDP approach requires an estimate of the measurement error, the precise distribution of measurement errors will itself be unknown in most cases.  However, the POMDP approach is quite robust to overestimation of the measurement error.  As an extreme example of this, we consider the case where the POMDP solution assumes the largest measurement error level considered here, $\sigma_m = 0.15$, while performing simulations in which measurements occur without error:

```{r overest}
set.seed(12345)
true <- 3 # sigma_g = 0.15, sigma_m = 0
source("../appendix/pomdp_overestimate.R")
pomdp_overest_sims <- 
  map2_dfr(models, alphas, function(.x, .y){
    pomdp_overestimates(transition = .x$transition, 
            model_observation = .x$observation, 
            reward = .x$reward, 
            discount = discount, 
            true_observation = models[[true]]$observation,
            x0 = x0,
            Tmax = Tmax,
            alpha = .y,
            reps = reps)$df %>% 
              mutate(method = "POMDP") # include a column labeling method
           },
           .id = "scenario"
  )
```



Combine the POMDP simulations results from over-estimating measurement error with the previous results from the static policies for comparison:

```{r}

overest <-
bind_rows(static_sims, pomdp_overest_sims) %>%
  left_join(meta, by = "scenario") %>%  ## include scenario information (sigmas; etc)
  mutate(state = states[state], action = actions[action]) %>%
  select(time, state, rep, method, sigma_m, sigma_g, value) %>%
  group_by(time, method, sigma_m, sigma_g) %>%
  summarise(mean = mean(state), sd = sd(state)) %>%
  filter(sigma_g == "0.15") %>% 
  ungroup()

overest %>% filter(method != "POMDP", sigma_m == "0")  %>%
  bind_rows(
  overest %>% filter(method == "POMDP", sigma_m == "0.15")) %>%
  select(-sigma_m, -sigma_g) %>%

  write_csv(file.path(log_dir, "overest_sims.csv"))
```

```{r}
read_csv(file.path(log_dir, "overest_sims.csv")) %>%
  ggplot(aes(time, mean, col=method, fill=method)) + 
  geom_line(lwd = 1) + 
  geom_ribbon(aes(ymax = mean + sd, ymin = mean-sd), col = NA, alpha = 0.1) +
  coord_cartesian(ylim = c(0, 1))


```

--------

# Policy plots



```{r}
policy_table <- tibble(state = 1:length(states)) %>%
  bind_cols(policies) %>%
  gather(policy, harvest, -state) %>%
  mutate(harvest = actions[harvest], state = states[state]) %>%
  mutate(escapement = state - harvest)

```

Note that when recruitment occurs before harvest, $f(x) - h$, to get to $X_t = B_{MSY}$ as fast as possible, we actually want to harvest a little bit when X is below $B_{MSY}$, so that rather than over-shooting $B_{MSY}$ (CEerministic) recruitment would land us right at it $B_{MSY}$.  This corresponds to constant escapement.  The discrete grid makes these appear slightly stepped.  

Note that the deterministic solution crosses the MSY solution at an observed value of $B_{MSY}$ (i.e. $K/2 = 0.5$).  TAC harvests are always smaller than MSY harvests, but unlike the deterministic optimal solution, TAC and MSY solutions never go to zero.  


```{r fig.width = 5, fig.height=7}
h_plot <- policy_table %>% 
  ggplot(aes(state, harvest, col=policy)) + 
  geom_line(lwd=1)  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))  +
  theme(legend.position = "none")
  

e_plot <- policy_table   %>% 
  ggplot(aes(state, escapement, col=policy)) + 
  geom_line(lwd=1) + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8)) +
  theme(legend.position = "bottom")

gridExtra::grid.arrange(h_plot, e_plot, layout_matrix = rbind(1,2))
```

When recruitment happens before harvest, escapement is $f(x_t) - h_t$, not $x_t - h_t$.  The effect of a continuous function map $f$ on a discrete grid is also visible as slight wiggles when we plot in terms of escapement instead of harvest (as is common in the optimal control literature in fisheries, e.g. @Sethi2005).  

Under the optimal solution, escapement is effectively constant at $B_{MSY} = 0.5$: for all states above a certain size the population is harvested back down to that size. Note that even stocks observed at states slightly below $B_{MSY} = 0.5$ achieve this target escapement, since we are following the classic Graham Shaeffer formulation here where we observe first, then recruitment happens before harvest, and thus we see the population at smaller size than we harvest it.  In classical escapement analysis, observations are usually indexed instead to occur immediately before harvests, and this inflection point occurs right at $B_{MSY}$.


### Policies under uncertainty: S = D

In strategies whose policies are shown in the above plots all ignore both stochasticity and measurement error.  If want to compare these to an MDP or POMDP policy, we must specify the level of uncertainty. In the absence of measurement uncertainty this is straight forward.  @Reed1979 essentially tells us that for small growth noise (satisfying or approximately satisfying Reed's self-sustaining condition) that the stochastic optimal policy is equal to the deterministic optimal policy.  We can confirm this numerically as follows.

First we grab the transition matrix we have already defined when `sigma_g = 0.1`, and then solve the Stochastic Dynamic Programming solution (SDP) for the (fully observed) Markov Decision Process (MDP) using the `MDPtoolbox` package:

```{r}
i <- meta %>% filter(sigma_g == 0.1, sigma_m ==0) %>%
  pull(scenario) %>% as.integer()
m <- models[[i]]
mdp <- MDPtoolbox::mdp_policy_iteration(m$transition, m$reward, discount) 
```

We plot the resulting policy in comparison with the other static policies, which shows that even for such a large noise we get a stochastic constant escapement equal to that of of the deterministic CE calculation: 

```{r}
bind_rows(policy_table,
  data_frame(state = states, 
             policy = "SDP",
             harvest = actions[mdp$policy],
             escapement = states - actions[mdp$policy])) %>% 
  ggplot(aes(state, escapement, col=policy)) + 
  geom_line(lwd=1)  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8)) + 
  labs(caption = "SDP solution when sigma_g = 0.1")
```

Repeating this larger stochastic growth `sigma_g = 0.15`, shows only slight deviation from CE ($S > D$, as @Reed1979 proves for sufficiently large noise to violate the self-sustaining criterion.  Technically since our noise model in normally distributed none of our populations are self-sustaining for infinite time, but as these simulations show, in practice that deviation is very small.)


```{r}
i <- meta %>% filter(sigma_g == 0.15, sigma_m ==0) %>%
  pull(scenario) %>% as.integer()
m <- models[[i]]
mdp <- MDPtoolbox::mdp_policy_iteration(m$transition, m$reward, discount) 

bind_rows(policy_table,
  data_frame(state = states, 
             policy = "SDP",
             harvest = actions[mdp$policy],
             escapement = states - actions[mdp$policy])) %>% 
  ggplot(aes(state, escapement, col=policy)) + 
  geom_line(lwd=1)  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8)) + 
  labs(caption = "SDP solution when sigma_g = 0.15")
```


### Comparing POMDP Policies

The comparison of POMDP policy is yet more complicated, but the POMDP policy cannot be expressed merely in terms of a target harvest (or escapement) level given an estimation of the stock size (state).  The optimal solution for the partially observed system must also reflect all prior observations of the system, not merely the most recent observation, as the system is not Markovian in the observed state variable.  We summarize this history as a prior "belief"" about the state, which is updated according to Bayes rule after each observation.  (Note that @Sethi2005 fails to realize this and plots solutions with measurement uncertainty without reference to the prior, which explains their counter-intuitive finding that increased uncertainty should result in increased harvest rates).  

Let us look at the POMDP solutions under various priors focusing on the case of moderate uncertainty, $\sigma_g = \sigma_m = 0.1$.  (Recall we have already solved the POMDP solution for this model in the simulations above, as defined by the `alpha` vectors, so we can quickly load that solution now.)

```{r}
i <- meta %>% 
  filter(sigma_g == 0.15, sigma_m == 0.1) %>% 
  pull(scenario) %>% as.integer()
m <- models[[i]]
alpha <- alphas[[i]] # we need the corresponding alpha vectors  
```

We will consider what the POMDP solution looks like under a few different prior beliefs. A uniform prior sounds like a conservative assumption, but it is not: it puts significantly more weight on improbably large stock values than other priors.  (Loading the $\alpha$ vectors from our POMDP solution computed earlier, we can then compute a POMDP given these $\alpha$, the matrices for transition, observation, and reward, and the prior we are using)

```{r}
unif_prior = rep(1, length(states)) / length(states) # initial belief
unif <- compute_policy(alpha, m$transition, m$observation, m$reward,  unif_prior)
```

For more realistic set of priors, we will consider priors centered at the target $B_{MSY}$ size (or $S^*$ in the language of Reed), at half $B_{MSY}$ (i.e. $K/4$), and at $\tfrac{3}{4} K$, each with a standard deviation of $\sigma_m = 0.1$ (i.e. the uncertainty around a single observation of a stock at that size.)

```{r}
prior_star <- m$observation[, which.min(abs(states - S_star)),1]
prior_low <- m$observation[,which.min(abs(states - 0.25 * K)),1]
prior_high <- m$observation[,which.min(abs(states - 0.75 * K)),1] 

star <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_star)
low <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_low)
high <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_high)
```


We gather these solutions into a single data frame and convert from grid indices to continuous values

```{r}
df <- unif
df$medium <- star$policy
df$low <- low$policy
df$high <- high$policy

pomdp_policies <- df %>% 
  select(state, uniform = policy, low, medium, high) %>% 
  gather(policy, harvest, -state) %>%
  mutate(state = states[state], 
         harvest = actions[harvest]) %>%
  mutate(escapement = state-harvest) %>%
  select(state, policy, harvest, escapement)

all_policies <- bind_rows(policy_table, pomdp_policies)
write_csv(all_policies, file.path(log_dir, "all_policies.csv"))
```

Plot these policies in terms of both Harvest and Escapement:

```{r}
recode_policies <- 
  all_policies %>%
  filter(policy %in% c("CE", "MSY", "TAC", "low", "high")) %>%
  mutate(policy = 
    fct_recode(policy, 
               "POMDP: low prior" = "low", 
               "POMDP: high prior" = "high"))
```


```{r}
recode_policies %>%
  ggplot(aes(state, escapement, col = policy)) + 
  geom_line(lwd=1) + 
  coord_cartesian(xlim = c(0,1), ylim=c(0,.8))
```

```{r}
recode_policies %>%
  ggplot(aes(state, harvest, col = policy)) + 
  geom_line(lwd=1) + 
  coord_cartesian(xlim = c(0,1), ylim=c(0,.6))
```



```{r, include=FALSE}
## Data for manuscript; combine prior distribution data with policy data
priors <- 
  data_frame(states, 
             prior_low,
             prior_star,
             unif_prior,
             prior_high) %>%
  select(state = states, 
         "POMDP: low prior" = prior_low, 
         "POMDP: high prior" = prior_high) %>%
  gather(policy, prior, -state) %>%
  mutate(prior = prior*10)
write_csv(priors, file.path(log_dir, "priors.csv"))
```

```{r}
p3 <- read_csv(file.path(log_dir, "priors.csv")) %>%
ggplot(aes(state, prior, col = policy)) + 
  geom_line(lwd=1) +
  coord_cartesian(xlim = c(0,1))
p3
```


```{r include = FALSE}
policy_df <- all_policies %>% 
  filter(policy %in% c("low", "high", "CE", "TAC")) %>%
  mutate(policy = fct_recode(policy,
                             "POMDP: low prior" = "low",
                             "POMDP: high prior" = "high"
                             )) 
policies_w_priors <- right_join(priors, policy_df, by = c("state", "policy"))

write_csv(policies_w_priors, file.path(log_dir, "policies_w_priors.csv"))

```


```{r fig.width=5, fig.height=7, include = FALSE}
policies_w_priors %>% 
  gather(panel, value, -state, -policy) %>%
  ggplot(aes(state, value, col=policy)) + 
  geom_line(lwd=1)  +
  facet_wrap(~panel, ncol=1, scales = "free_y") +
  coord_cartesian(xlim = c(0,1), ylim = c(0,.8)) + 
  theme(legend.position = "bottom")
```




