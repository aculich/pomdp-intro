---
title: "POMDP comparisons across sigma_m, sigma_g"
author: "Carl Boettiger"
date: "`r Sys.Date()`"
output: 
  github_document: default
  pdf_document: default
---


```{r aethetics, include=FALSE}
knitr::opts_chunk$set(comment="", message=FALSE, cache=TRUE)
ggplot2::theme_set(ggplot2::theme_bw())
```


```{r message=FALSE}
# devtools::install_github("boettiger-lab/sarsop")  ## install package first if necessary.
library(sarsop)       # the main POMDP package
library(tidyverse)    # for munging and plotting
library(parallel)
```

```{r}
options(mc.cores=parallel::detectCores())
```



## Basic deterministic model

```{r}
r <- 0.75
K <- 1

## Unlike classic Graham-Schaefer, this assumes harvest occurs right after assessment, before recruitment
f <- function(x, h){ 
    s <- pmax(x-h, 0) 
    s + s * r * (1 - s / K) 
}
```

Utility (reward) function:

```{r}
reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```


## Calculating MSY

This uses a generic optimization routine to find the stock size at which the maximum growth rate is achieved.  

```{r}
## A generic routine to find stock size (x) which maximizes 
## growth rate (f(x,0) - x, where x_t+1 = f(x_t))
S_star <- optimize(function(x) -f(x,0) + x / discount, 
                   c(0, 2*K))$minimum
```


Since we `observe` -> `harvest` -> `recruit`, we would observe the stock at its pre-harvest size, $X_t \sim B_{MSY} + H_{MSY}$.

```{r}
#B_MSY <- S_star     # recruit first, as in classic Graham-Schaefer
B_MSY <- f(S_star,0) # harvest first, we observe the population at B_msy + h

#MSY <- f(B_MSY,0) - B_MSY  # recruit first
MSY <- B_MSY - S_star  # harvest first

F_MSY <- MSY / B_MSY  
F_PGY = 0.8 * F_MSY
```


As a basic reference point, simulate these three policies in a purely deterministic world.  Unlike later simulations, here we consider all states an actions exactly (that is, within floating point precision).  Later, states and actions are limited to a discrete set, so solutions can depend on resolution and extent of that discretization. 




```{r}

msy_policy <- function(x) F_MSY * x
pgy_policy <- function(x) F_PGY * x

## ASSUMES harvest takes place *before* recruitment, f(x-h), not after.
escapement_policy <- function(x) pmax(x - S_star,0)  
x0 <- K/6
Tmax = 20
det_sim <- function(policy, f, x0, Tmax){
    action <- state <- obs <- as.numeric(rep(NA,Tmax))
    state[1] <- x0

    for(t in 1:(Tmax-1)){
      action[t] <- policy(state[t])
      obs[t] <- state[t] - action[t]  # if we observe after harvest but before recruitment
      state[t+1] <- f(state[t], action[t]) 
    }
    data.frame(time = 1:Tmax, state, action, obs)
  }

sims <- 
  list(msy = msy_policy, 
       pgy = pgy_policy,
       det = escapement_policy) %>% 
  map_df(det_sim, 
         f, x0, Tmax, 
         .id = "method") 
sims %>%
  ggplot(aes(time, state, col=method)) + 
  geom_line() + 
  geom_hline(aes(yintercept=B_MSY), lwd=1)

```



-----------


##  Introduce a discrete grid

```{r}
## Discretize space
states <- seq(0,2, length=100)
actions <- states
observations <- states
```

We compute the above policies on this grid for later comparison.


```{r}
index <- function(x, grid) map_int(x, ~ which.min(abs(.x - grid)))


policies <- data.frame(
  det = index(pmax(f(states,0) - S_star,0), actions), # assumes obs->recruit->harvest, f(x_t) - h_t
 # det = index(pmax(states - S_star,0),     actions), # assumes obs->harvest->recruit, f(x_t - h_t)
  msy = index(states * F_MSY,               actions),
  pgy = index(states * F_PGY,               actions))
```



# POMDP Model

We compute POMDP matrices for a range of `sigma_g` and `sigma_m` values:

```{r}
meta <- expand.grid(sigma_g = c(0.02, 0.1, 0.15), 
                    sigma_m = c(0, 0.1, 0.15),
                    stringsAsFactors = FALSE) %>%
        mutate(scenario  = as.character(1:length(sigma_m)))
log_dir <- "appendix_alphas_harvestfirst" # Store the computed solution files here
meta
```


```{r}
models <- 
  parallel::mclapply(1:dim(meta)[1], 
           function(i){
  fisheries_matrices(
  states = states,
  actions = actions,
  observed_states = observations,
  reward_fn = reward_fn,
  f = f,
  sigma_g = meta[i,"sigma_g"][[1]],
  sigma_m = meta[i,"sigma_m"][[1]],
  noise = "normal")
})

```


## POMDP solution

The POMDP solution is represented by a collection of alpha-vectors and values, returned in a `*.policyx` file.
Each scenario (parameter combination of `sigma_g`, `sigma_m`, and so forth) results in a separate solution file.

Because this solution is computationally somewhat intensive, be sure to have ~ 4 GB RAM per core if running the 9 models in parallel.  Alternately, readers can skip the evaluation of this code chunk and read the cached solution from the `policyx` file using the `*_from_log` functions that follow:

```{r}
options(mc.cores=9)
```

```{r eval=TRUE}
dir.create(log_dir)

## POMDP solution (slow, >10,000 seconds per scenario, & memory intensive)
system.time(
  alphas <- 
    parallel::mclapply(1:length(models), 
    function(i){
      log_data <- data.frame(model = "gs", 
                             r = r, 
                             K = K, 
                             sigma_g = meta[i,"sigma_g"][[1]], 
                             sigma_m = meta[i,"sigma_m"][[1]], 
                             noise = "normal",
                             scenario = meta[i, "scenario"][[1]])
      
      sarsop(models[[i]]$transition,
             models[[i]]$observation,
             models[[i]]$reward,
             discount = discount,
             precision = 0.00000002,
             timeout = 15000,
             log_dir = log_dir,
             log_data = log_data)
    })
)

```

We can read the stored solution from the log:

```{r}
meta <- meta_from_log(data.frame(model="gs"), log_dir) %>% left_join(meta) %>% arrange(scenario)
alphas <- alphas_from_log(meta, log_dir)
```


## Simulating the static policies under uncertainty

```{r static_sims}
Tmax <- 100
x0 <- which.min(abs(K/6 - states))
reps <- 100

static_sims <- 
 map_dfr(models, function(m){
            do_sim <- function(policy) sim_pomdp(
                        m$transition, m$observation, m$reward, discount, 
                        x0 = x0, Tmax = Tmax, policy = policy, reps = reps)$df
            map_dfr(policies, do_sim, .id = "method")
          }, .id = "scenario") 

```

## Simulating the POMDP policies under uncertiainty

```{r pomdp_sims}
# slower!
unif_prior <- rep(1, length(states)) / length(states)
pomdp_sims <- 
  map2_dfr(models, alphas, function(.x, .y){
             sim_pomdp(.x$transition, .x$observation, .x$reward, discount, 
                       unif_prior, x0 = x0, Tmax = Tmax, alpha = .y,
                       reps = reps)$df %>% 
              mutate(method = "pomdp") # include a column labeling method
           },
           .id = "scenario")
```


Combine the resulting data frames

```{r}
sims <- bind_rows(static_sims, pomdp_sims) %>%
    left_join(meta) %>%    ## combine output with scenarios
    mutate(state = states[state], action = actions[action]) # index -> value

write_csv(sims, "appendix_harvestfirst_files/sims.csv")
```


##  Figure S1

We the results varying over different noise intensities, sigma_g, and sigma_m.  Figure 1 of the main text considers the case of sigma_g = 0.05, sigma_m = 0.1

```{r}
sims %>%
  select(time, state, rep, method, sigma_m, sigma_g) %>%
  group_by(time, method, sigma_m, sigma_g) %>%
  summarise(mean = mean(state), sd = sd(state)) %>%
  ggplot(aes(time, mean, col=method, fill=method)) + 
  geom_line() + 
  geom_ribbon(aes(ymax = mean + sd, ymin = mean-sd), col = NA, alpha = 0.2) +
  facet_grid(sigma_m ~ sigma_g)
```



## Policy plots



```{r}
policy_table <- tibble(state = 1:length(states)) %>%
  bind_cols(policies) %>%
  gather(policy, harvest, -state) %>%
  mutate(harvest = actions[harvest], state = states[state]) %>%
  mutate(escapement = state - harvest)

write_csv(policy_table, "appendix_files/policy_table.csv")
```

Note that when recruitment occurs before harvest, $f(x) - h$, to get to $X_t = B_{MSY}$ as fast as possible, we actually want to harvest a little bit when X is below $B_{MSY}$, so that rather than over-shooting $B_{MSY}$ (deterministic) recruitment would land us right at it $B_{MSY}$.  This corresponds to constant escapement.  The discrete grid makes these appear slightly stepped.  

Note that the deterministic solution crosses the MSY solution at an observed value of $B_{MSY}$ (i.e. $K/2 = 0.5$).  PGY harvests are always smaller than MSY harvests, but unlike the deterministic optimal solution, PGY and MSY solutions never go to zero.  



```{r}
policy_table %>% 
  ggplot(aes(state, harvest, col=policy)) + 
  geom_line()  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))
```

Note that when recruitment happens before harvest, escapement is $f(x_t) - h_t$, not $x_t - h_t$.  The effect of a continuous function map $f$ on a discrete grid is also visible as slight wiggles when we plot in terms of escapement instead of harvest (as is common in the optimal control literature in fisheries, e.g. @Sethi2005).  

Note that under the optimal solution, escapement is effectively constant at $B_{MSY} = 0.5$: for all states above a certain size the population
is harvested back down to that size. Note that even stocks observed at states slightly below $B_{MSY} = 0.5$ achieve this target escapement, since we are following the classic Graham Shaeffer formulation here where we observe first, then recruitment happens before harvest, and thus we see the population at smaller size than we harvest it.  In classical escapement analysis, observations are usually indexed instead to occur immediately before harvests, and this inflection point occurs right at $B_{MSY}$.

```{r}
policy_table   %>% 
  ggplot(aes(state, escapement, col=policy)) + 
  geom_line() + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))

```


### Policies under uncertainty

In strategies whose policies are shown in the above plots all ignore both stochasticity and measurement error.  If want to compare these to an MDP or POMDP policy, we must specify the level of uncertainty. 

In the absence of measurement uncertainty this is straight forward.  @Reed1979 essentially tells us that for small growth noise (satisfying or approximately satisfying Reed's self-sustaining condition) that the stochastic optimal policy is equal to the deterministic optimal policy.  We can confirm this numerically as follows.

First we grab the transition matrix we have already defined for small `sigma_g`: 

```{r}
i <- meta %>% filter(sigma_g == 0.02, sigma_m ==0) %>% pull(scenario) %>% as.integer()
m <- models[[i]]
```

With no observation uncertainty, we can solve numerically for the optimal policy with stochastic dynamic programming

```{r}
mdp <- MDPtoolbox::mdp_policy_iteration(m$transition, m$reward, discount) 
```

Adding this to the plot we see the result is identical to the deterministic case:

```{r}
bind_rows(policy_table,
  data.frame(state = states, 
             policy = "sdp 0.02",
             harvest = actions[mdp$policy],
             stringsAsFactors = FALSE) %>%
  mutate(escapement = state- harvest)) %>% 
  ggplot(aes(state, harvest, col=policy)) + 
  geom_line()  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))
```


Repeating this for larger stochasticity, we get a slightly more conservative result:

```{r}
i <- meta %>% filter(sigma_g == 0.1, sigma_m ==0) %>% pull(scenario) %>% as.integer()
m <- models[[i]]
mdp <- MDPtoolbox::mdp_policy_iteration(m$transition, m$reward, discount) 
```


```{r}
bind_rows(policy_table,
  data.frame(state = states, 
             policy = "sdp 0.1",
             harvest = actions[mdp$policy],
             stringsAsFactors = FALSE)) %>% 
  ggplot(aes(state, harvest, col=policy)) + 
  geom_line()  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))
```

### Comparing POMDP Policies

The comparison of POMDP policy is yet more complicated, but the POMDP policy cannot be expressed merely in terms of a target harvest (or escapement) level given an estimation of the stock size (state).  The optimal solution for the partially observed system must also reflect all prior observations of the system, not merely the most recent observation, as the system is not Markovian in the observed state variable.  We summarize this history as a prior "belief"" about the state, which is updated according to Bayes rule after each observation.  (Note that @Sethi2005 fails to realize this and plots solutions with measurement uncertainty without reference to the prior, which explains their counter-intuitive finding that increased uncertainty should result in increased harvest rates).  

Let us look at the POMDP solutions under various priors focusing on the case of moderate uncertainty, $\sigma_g = \sigma_m = 0.1$.  (Recall we have already solved the POMDP solution for this model in the simulations above, as defined by the `alpha` vectors, so we can quickly load that solution now.)

```{r}
i <- meta %>% filter(sigma_g == 0.1, sigma_m ==0.1) %>% pull(scenario) %>% as.integer()
m <- models[[i]]
alpha <- alphas[[i]] # we need the corresponding alpha vectors
```

We will consider what the POMDP solution looks like under a few different prior beliefs. A uniform prior sounds like a conservative assumption, but it is not: it puts significantly more weight on improbably large stock values than other priors.  (Loading the $\alpha$ vectors from our POMDP solution computed earlier, we can then compute a POMDP given these $\alpha$, the matrices for transition, observation, and reward, and the prior we are using)

```{r}
unif_prior = rep(1, length(states)) / length(states) # initial belief
unif <- compute_policy(alpha, m$transition, m$observation, m$reward,  unif_prior)

```

For more realistic set of priors, we will consider priors centered at the target $B_{MSY}$ size (or $S^*$ in the language of Reed), at half $B_{MSY}$, and at 1.5 times $B_{MSY}$, each with a standard deviation of $\sigma_m = 0.1$ (i.e. the uncertainty around a single observation of a stock at that size.)

```{r}
i_star <- which.min(abs(states - S_star))
i_low <- which.min(abs(states - 0.5 * S_star))
i_high <- which.min(abs(states - 1.5 * S_star))

prior_star <- m$observation[,i_star,1]
prior_low <- m$observation[,i_low,1]
prior_high <- m$observation[,i_high,1] 

star <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_star)
low <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_low)
high <- compute_policy(alpha, m$transition, m$observation, m$reward,  prior_high)

```


We gather these solutions into a single data frame and convert from grid indices to continuous values

```{r}
df <- unif
df$medium <- star$policy
df$low <- low$policy
df$high <- high$policy

pomdp_policies <- df %>% 
  select(-value) %>% 
  rename(uniform = policy) %>% 
  gather(policy, harvest, -state) %>%
  mutate(state = states[state], 
         harvest = actions[harvest]) %>%
  mutate(escapement = state-harvest) %>%
  select(state, policy, harvest, escapement)

```


```{r}
pomdp_policies %>% 
  ggplot(aes(state, harvest, col=policy)) + 
  geom_line()  + 
  coord_cartesian(xlim = c(0,K), ylim = c(0, .8))
```
